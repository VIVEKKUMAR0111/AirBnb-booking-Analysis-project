{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PBTbrJXOngz2",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIVEKKUMAR0111/AirBnb-booking-Analysis-project/blob/main/VIVEK_ML_PROJECT_Yes_Bank_Stock_Closing_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - desk_champions\n",
        "##### **Team Member 1 -** Sanvedana Patil\n",
        "##### **Team Member 2 -** Vivek Kumar\n",
        "##### **Team Member 3 -** Omkar Naik\n",
        "##### **Team Member 4 -** Harshal kamble"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stock's closing price of the month."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/VIVEKKUMAR0111\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM STATEMENT :-\n",
        " This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month.\n",
        "Our main objective is to predict the stockâ€™s closing price of the month."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QvF7ktzcn53G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path = '/content/drive/MyDrive/data_YesBank_StockPrices.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "hn3PA6a7pIk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Taking a look at the data.\n",
        "df.head()          # displays first five instances of the dataframe.\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are trying to track variation in stock price on different dates, it makes sense to set this column as index.\n"
      ],
      "metadata": {
        "id": "xh5AlvUFxkwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('Date', inplace=True)           # setting Date column as index."
      ],
      "metadata": {
        "id": "YfnPTiAWxs-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() "
      ],
      "metadata": {
        "id": "wXyxYaoRxviQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "# So there are no null values in our dataset.\n",
        "# Getting information about our data - its datatypes, its size etc. also printing the shape of the data.\n",
        "df.info()\n",
        "print('\\n', f'The shape of the dataset is : {df.shape}')"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I knowing only opening price , closing price ,lower price and Higher price."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining the data:-\n",
        "We have a dataset containing values of Yes bank monthly stock prices as mentioned in our problem statement. \n",
        "\n",
        "Explaining the features present :-\n",
        "\n",
        "\n",
        "*  **Date :-** The date (Month and Year provided)\n",
        "*  **Open :-** The price of the stock at the beginning of a particular time period.\n",
        "*  **High :-**The Peak(Maximum) price at which a stock traded during the period.\n",
        "*  **Low :-**The Lowest price at which a stock traded during the period.\n",
        "*  **Close :-** The trading price at the end (in this case end of the month)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the dataframe above, all the columns we have contain numerical data. There is no categorical data present.\n"
      ],
      "metadata": {
        "id": "GR6etyFkxONi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now preserve the original data before we operate on it.\n",
        "preserved_stock_data = df.copy()"
      ],
      "metadata": {
        "id": "z_cgPB01yAj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate instances.\n",
        "df[df.duplicated()==True]"
      ],
      "metadata": {
        "id": "VEzSFLSuxPkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So there is no duplicate data in our dataframe.\n",
        "# checking the datatypes once more.\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "ADWi7EK4ykGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all features for presence of outliers.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  sns.boxplot(df[col])\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "z5RqN9-c28_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are some outliers present in our data. We will need to deal with these before proceeding to modelling."
      ],
      "metadata": {
        "id": "sh6UNoL5_XdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the dependent and independent variables.\n",
        "independent_variables = df.columns.tolist()[:-1]\n",
        "dependent_variable = ['Close']\n",
        "\n",
        "print(independent_variables)\n",
        "print(dependent_variable)"
      ],
      "metadata": {
        "id": "-hfC3FdD_ekd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the dependent variable .\n",
        "plt.figure(figsize=(12,7))\n",
        "df['Close'].plot(color = 'r')\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='green')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Price with Date')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vbS5M9G6_jiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the stock price is rising up until 2018 when the fraud case involving Rana Kapoor happened after which the stock price has had a sharp decline."
      ],
      "metadata": {
        "id": "xn1pE0GuACLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distributions of all features.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(df[col], color='y')\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(df[col].mean(),color='green',linewidth=2)                           \n",
        "  # axvline plots a vertical line at a value (mean in this case). \n",
        "  plt.axvline(df[col].median(),color='red',linestyle='dashed',linewidth=1.5)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-hiQkgqxADTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that ***these distributions are positively skewed***. The mean and median are at significant distance from each other.\n",
        "\n",
        " So we need to transform them into something close to a Normal Distribution as our models give optimal results that way."
      ],
      "metadata": {
        "id": "gwI4yuDQATGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets use log transformation on these features using np.log() and plot them.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(np.log10(df[col]), color='y')\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  # Plotting the mean and the median.\n",
        "  plt.axvline(np.log10(df[col]).mean(),color='green',linewidth=2) \n",
        "  plt.axvline(np.log10(df[col]).median(),color='red',linestyle='dashed',linewidth=1.5)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7PZbgPBEAbvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the distributions are very similar to Normal distribution. The mean and median values are nearly same."
      ],
      "metadata": {
        "id": "qwY4F44BAlVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check for outliers now in the transformed variable data.\n",
        "for col in df.columns:\n",
        "  plt.figure(figsize=(7,7))\n",
        "  sns.boxplot(np.log10(df[col]))\n",
        "  plt.xlabel(col, fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "q2-BDzSKAn18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have no outliers anymore. Log transformation diminishes the outlier's effect.\n",
        "\n",
        "Since we have a very small dataset to work with, dropping the outliers completely is not a good idea. So this is how we are going to leave them."
      ],
      "metadata": {
        "id": "vgBVJriZA1yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all of our independent variables are highly correlated to the dependent variable.\n",
        "\n",
        "And the relationship between dependent and independent variables is linear in nature."
      ],
      "metadata": {
        "id": "AVD2MTD_CV6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's visualise for the correlation among all variables.\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(16,7))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "M-yaM356BH_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap above, we can clearly see that there is a very high correlation between each pair of features in our dataset. While it is desirable for the dependent variable to be highly correlated with independent variables, the independent varibles should ideally not have high correlation with one another."
      ],
      "metadata": {
        "id": "W9OMb-CKCfFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This causes a problem for us as high correlation among independent variables (multicollinearity) is a problem for our models."
      ],
      "metadata": {
        "id": "XtQeyrzlCjg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualise the relationship between each pair of variables using pair plots.\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "Vxfh_EHGBIC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zxkPtrYNCtYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dealing with multicollinearity using VIF analysis.\n",
        "# Calculating VIF(Variation Inflation Factor) to see the correlation between independent variables\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "iQ5-kWSvBIIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Date','Close']]])"
      ],
      "metadata": {
        "id": "V_8-wbLPBIMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the values of VIF factor are very high. However since the dataset is so small and has just 3 independent features, multicollinearity is unavoidable here as any feature engineering will lead to loss of information."
      ],
      "metadata": {
        "id": "if-f5ZTDD8eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating arrays of our input variable and label to feed the data to the model.\n",
        "# Create the data of independent variables\n",
        "x = np.log10(df[independent_variables]).values           \n",
        " # applying log transform on our independent variables.\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = np.log10(df[dependent_variable]).values              \n",
        " # applying log transform on our dependent variable."
      ],
      "metadata": {
        "id": "TrzG5wweD-zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into a train and a test set. we do this using train test split.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)       # we keep 20% of the data in test set."
      ],
      "metadata": {
        "id": "3NjqWqL6EHw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the data is very important for us so as to avoid giving more importance to features with large values. This is achieved by normalization or standardization of the data."
      ],
      "metadata": {
        "id": "cRpO1Q_7EL-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "bTWDorAfERy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the values.\n",
        "x_train[0:10]"
      ],
      "metadata": {
        "id": "A8l6agybEY6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Linear Regression**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NxmKhLfEEnJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing LinearRegression model and the metrics that we will use for evaluating different models performance.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "FITtm_EuElbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model.\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Fitting the model on our train data.\n",
        "model_lr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "6PWKbwypEzsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on our test data.\n",
        "y_pred_linear = model_lr.predict(x_test)"
      ],
      "metadata": {
        "id": "HJhWCdJlE4FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters. printing the intercept.\n",
        "model_lr.intercept_"
      ],
      "metadata": {
        "id": "54rh21CiE7xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the model coefficients.\n",
        "model_lr.coef_"
      ],
      "metadata": {
        "id": "XP0xgjzXE_Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the performance metrics.\n",
        "MAE_linear = round(mean_absolute_error(10**(y_test),(10**y_pred_linear)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_linear}\")\n",
        "\n",
        "MSE_linear = round(mean_squared_error((10**y_test),10**(y_pred_linear)),4)\n",
        "print(f\"Mean squared Error : {MSE_linear}\")\n",
        "\n",
        "RMSE_linear = round(np.sqrt(MSE_linear),4)\n",
        "print(f\"Root Mean squared Error : {RMSE_linear}\")\n",
        "\n",
        "R2_linear = round(r2_score(10**(y_test), 10**(y_pred_linear)),4)\n",
        "print(f\"R2 score : {R2_linear}\")\n",
        "\n",
        "Adjusted_R2_linear = round(1-(1-r2_score(10**y_test,10**y_pred_linear))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),3)\n",
        "print(f\"Adjusted R2 score : {Adjusted_R2_linear}\")"
      ],
      "metadata": {
        "id": "sBR37OlSFA51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual and predicted test data.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Linear regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HfQqjKPcFIZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to store our performance data for this model so that we can compare them with other models. Let's store them in a dict for now."
      ],
      "metadata": {
        "id": "j94oqsTqFPKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regessor_list = {'Mean Absolute Error' : MAE_linear,'Mean squared Error' : MSE_linear,\n",
        "                   'Root Mean squared Error' : RMSE_linear,'R2 score' : R2_linear,'Adjusted R2 score' : Adjusted_R2_linear }\n",
        "                   "
      ],
      "metadata": {
        "id": "EbmY0y1UFRBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting above dict into a dataframe\n",
        "metric_df = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()"
      ],
      "metadata": {
        "id": "RJn0yp6YFpVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming the columns.\n",
        "metric_df = metric_df.rename(columns={'index':'Metric',0:'Linear Regression'})\n",
        "metric_df"
      ],
      "metadata": {
        "id": "IfheY_icFu-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use this df to store all metrics of all other models so we can easily compare them."
      ],
      "metadata": {
        "id": "Sc2e3TezF-5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Lasso Regression** with cross validated regularization.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "c7zZmyqQGFDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Lasso model.\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "CwTuWmJnFz1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with some base values.\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000)\n",
        "# Fitting the model on our training data.\n",
        "lasso.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "nmLQNDtKFz96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the intercept and coefficients.\n",
        "lasso.intercept_"
      ],
      "metadata": {
        "id": "48Xi74-tF0D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "0Jbw0ywIF0Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation. optimizing our model by finding the best value of our hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.005,0.006,0.007,0.01,0.015,0.02,1e-1,1,5,10,20,30,40,45,50]}  # list of parameters. \n",
        "                                                                                  \n",
        "lasso_regressor = GridSearchCV(lasso, lasso_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "EnpRXAjWF0K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "lasso_regressor.best_params_         \n",
        "# after several iterations and trials, we get this value as best parameter value."
      ],
      "metadata": {
        "id": "sPF1ccVjGlgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score\n",
        "lasso_regressor.best_score_\n"
      ],
      "metadata": {
        "id": "rqJ4g6t4Glph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on the test dataset.\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "print(y_pred_lasso)"
      ],
      "metadata": {
        "id": "4e0JY8aeGuYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the performance using evaluation metrics.\n",
        "MAE_lasso = round(mean_absolute_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_lasso}\")\n",
        "\n",
        "MSE_lasso  = round(mean_squared_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(\"Mean squared Error :\" , MSE_lasso)\n",
        "\n",
        "RMSE_lasso = round(np.sqrt(MSE_lasso),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_lasso)\n",
        "\n",
        "R2_lasso = round(r2_score(10**(y_test), 10**(y_pred_lasso)),4)\n",
        "print(\"R2 score :\" ,R2_lasso)\n",
        "\n",
        "Adjusted_R2_lasso = round(1-(1-r2_score(10**y_test, 10**y_pred_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_lasso)"
      ],
      "metadata": {
        "id": "G6tk22_aG2CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now saving these metrics to our metrics dataframe. First we save them in a list and then we pass them to the df.\n",
        "metric_df['Lasso'] = [MAE_lasso, MSE_lasso, RMSE_lasso, R2_lasso, Adjusted_R2_lasso]"
      ],
      "metadata": {
        "id": "g3V-fnawG551"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the predicted values vs actual.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Lasso regression\")"
      ],
      "metadata": {
        "id": "jI9jTcegG59I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Ridge Regression** with cross validated regularization.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fk7xyTuUHBKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing ridge regressor model.\n",
        "from sklearn.linear_model import Ridge    \n",
        "ridge = Ridge()         # iitializing the model\n",
        "\n",
        "# initiating the parameter grid for alpha (regularization strength).\n",
        "ridge_param_grid = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,0.3,0.7,1,1.2,1.33,1.365,1.37,1.375,1.4,1.5,1.6,1.8,2.5,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# cross validation. \n",
        "ridge_regressor = GridSearchCV(ridge, ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "uOBvJmTWHHmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter value (for alpha)\n",
        "ridge_regressor.best_params_"
      ],
      "metadata": {
        "id": "LckcUrZXHKN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score for optimal value of alpha.\n",
        "ridge_regressor.best_score_"
      ],
      "metadata": {
        "id": "sYcRfpNmHNkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the test dataset now.\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "of8Zvc4wHUim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating performance.\n",
        "MAE_ridge = round(mean_absolute_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = round(mean_squared_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(\"Mean squared Error :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = round(np.sqrt(MSE_ridge),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_ridge)\n",
        "\n",
        "R2_ridge = round(r2_score(10**(y_test), 10**(y_pred_ridge)),4)\n",
        "print(\"R2 score :\" ,R2_ridge)\n",
        "\n",
        "Adjusted_R2_ridge = round(1-(1-r2_score(10**y_test, 10**y_pred_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_ridge)"
      ],
      "metadata": {
        "id": "cTIPoTyoHY2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these values in a list and appending to our metric df.\n",
        "ridge_regressor_list = [MAE_ridge,MSE_ridge,RMSE_ridge,R2_ridge,Adjusted_R2_ridge]\n",
        "metric_df['Ridge'] = ridge_regressor_list"
      ],
      "metadata": {
        "id": "dB29p5OjHc_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predicted and actual target variable values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Ridge regression\")"
      ],
      "metadata": {
        "id": "tnDKxmvcHg-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Elastic-Net Regression** with cross validation."
      ],
      "metadata": {
        "id": "5-bkkB8pHoAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and initializing Elastic-Net Regression.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "\n",
        "# initializing parameter grid.\n",
        "elastic_net_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.001,0.01,0.02,0.03,0.04,1,5,10,20,40,50,60,100],\n",
        "                          'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
        "\n",
        "# cross-validation.\n",
        "elasticnet_regressor = GridSearchCV(elasticnet_model, elastic_net_param_grid, scoring='neg_mean_squared_error',cv=5)\n",
        "elasticnet_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "Km_XYLd-HlZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter\n",
        "elasticnet_regressor.best_params_"
      ],
      "metadata": {
        "id": "iQUd9jPRHu7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best score for the optimal parameter.\n",
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "1EB5_zPxHvBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions.\n",
        "y_pred_elastic_net = elasticnet_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "lqaXMg2rHvKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE_elastic_net = round(mean_absolute_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_elastic_net}\")\n",
        "\n",
        "MSE_elastic_net  = round(mean_squared_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(\"Mean squared Error :\" , MSE_elastic_net)\n",
        "\n",
        "RMSE_elastic_net = round(np.sqrt(MSE_elastic_net),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_elastic_net)\n",
        "\n",
        "R2_elastic_net = round(r2_score(10**(y_test), (10**y_pred_elastic_net)),4)\n",
        "print(\"R2 score :\" ,R2_elastic_net)\n",
        "\n",
        "Adjusted_R2_elastic_net = round(1-(1-r2_score(10**y_test, 10**y_pred_elastic_net))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_elastic_net)"
      ],
      "metadata": {
        "id": "KoiUzkAJH68h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these metrics in our dataframe.\n",
        "elastic_net_metric_list = [MAE_elastic_net,MSE_elastic_net,RMSE_elastic_net,R2_elastic_net,Adjusted_R2_elastic_net]\n",
        "metric_df['Elastic Net'] = elastic_net_metric_list"
      ],
      "metadata": {
        "id": "F3PQPir3IEIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us plot the actual and predicted target variables values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Elastic Net regression\")"
      ],
      "metadata": {
        "id": "VtL13KanIELc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the performance of all models that we have implemented.\n",
        "metric_df"
      ],
      "metadata": {
        "id": "c6A6KhyLIEQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the predicted values of all the models against the true values.\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(10**y_test, linewidth=2)\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.legend(['linear','lasso','ridge','elastic_net'])\n",
        "plt.title('Actual vs Predicted Closing Price values by various Algorithms', weight = 'bold',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "chdMgZUbIET0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above graph, all of our models are performing really well and are able to closely approximate the actual values."
      ],
      "metadata": {
        "id": "1EXPo3QDIP91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the actual and elastic net predicted target variables values in a dataframe. \n",
        "actual_pred_df = pd.DataFrame(10**y_test,10**y_pred_elastic_net).reset_index().rename(columns = {'index':'Actual values',0:'Elastic Net Predicted values'})\n",
        "actual_pred_df.head()"
      ],
      "metadata": {
        "id": "EDPIU4AlIEcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.**\n",
        "*   **After loading the dataset, we found that there are no null values in our dataset nor any duplicate data.**\n",
        "*   **There are some outliers in our features however this being a very small dataset, dropping those instances will lead to loss of information.**\n",
        "*   **We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.**\n",
        "*   **There is a high correlation between the dependent and independent variables. This is a signal that our dependent variable is highly dependent on our features and can be predicted accurately from them.**\n",
        "*   **We found that there is a rather high correlation between our independent variables. This multicollinearity is however unavoidable here as the dataset is very small.**\n",
        "*   **We implemented several models on our dataset in order to be able to predict the closing price and found that all our models are performing remarkably well and *Elastic Net regressor is the best performing model with Adjusted R2 score value of 0.9932* and scores well on all evaluation metrics.** \n",
        "*   **All of the implemented models performed quite well on our data giving us the Adjusted R-square of over 99%.**\n",
        "*   **We checked for presence of Heterodasceticity in our dataset by plotting the residuals against the Elastic Net model predicted value and found that there is no Heterodasceticity present. Our model is performing well on all data-points.**\n",
        "*   **With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data.**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}